library(data.table)#data.table is an R package that provides an enhanced version of data.frame
#especially with much faster data reading speed
library(caret) #caret contains many powerful tools for data pre-processing and feature selection
library(dplyr) #dplyr provides flexible grammar and powerful tools of data manipulation
train = fread('train100k.csv',integer64 = 'numeric')
test = fread('test100k.csv',integer64 = 'numeric')
str(train)

prop.table(table(train$HasDetections)) #make sure it's balanced

train1 = train[,- "MachineIdentifier"]
test1 = test[,- "MachineIdentifier"]

n = colSums(is.na(train1) > 0) # NA entries
m = colSums(train1 == '', na.rm = T) # Blank cells
(m+n)/100000 # We want to know the proportion of the missing value

miss_too_many=which((m+n)/100000>0.5) # We set the cutoff by 50%
train2 = subset(train1,select=-miss_too_many)
#delete those columns with too much missing data
test2=subset(test1,select=-miss_too_many)
#We can only apply with the criteria we get from the training data

y <- train$HasDetections # We will store our response variable in an individual column.
fea <- names(train2)
cats <- c()
cat_ind <- c()
for (f in colnames(train2))
  if (is.character(train2[[f]])|grepl(".Identifier", f)>0 )#Check is.character or include "Identifier"
  {cats <- c(cats, f) # Store the feature name
  cat_ind <- c(cat_ind, which(colnames(train2)==f))}#Store the column number of those features

uniquevalues <- c()
for (f in cat_ind)
{uniquevalues<- c(uniquevalues,length(unique(train2[[f]])))}

less_level_index=which(uniquevalues<5)
many_level_index=which(uniquevalues>=5)

#Remember the objective is to predict the label in the test data.
#Thus you cannot use them when you build the model
#We can only apply with the criteria we get from the training data.
cats[many_level_index]

tr=train2
ts=test2
for (i in many_level_index)
{
  col <- data.frame("predictor" = train2[[cat_ind[i]]], "target" = y) #create a new dataframe
  col_ts<- data.frame("predictor" = test2[[cat_ind[i]]]) # We won't use the test target
  lookup = col %>% #This part is using the dplyr package
    group_by(predictor) %>% #use groupby() to combine each level
    summarise(mean_target = mean(target)) #computing the target mean of each level
  col = left_join(col, lookup) #join the table with our computed mean
  col_ts = left_join(col_ts, lookup)
  tr[[cat_ind[i]]] <- col$mean_target #replace the value in the original table with computed mean
  ts[[cat_ind[i]]] <- col_ts$mean_target
}
str(tr)

#Remember the objective is to predict the label in the test data.
#Thus you cannot use them when you build the model
cats[less_level_index]

tr2=tr
ts2=ts
for (i in less_level_index)
{
  col <- data.frame("predictor" = tr2[[cats[i]]], "target" = y) # create a new data frame
  col_ts <- data.frame("predictor" = ts2[[cats[i]]], "target" = 0)
  # Here for test data we didn't use y, just use a zero column to stand on the place
  # We actually did not use "target", and we will delete this column afterwards
  col$predictor = as.factor(col$predictor)
  colnames(col) <- c(cats[i], "target") #get the original column name
  colnames(col_ts) <- c(cats[i], "target")
  dummy.vars = dummyVars(~ ., data = col, fullRank = TRUE)
  col.dummy = predict(dummy.vars, newdata = col) #create dummy (0-1) columns
  col_ts.dummy = predict(dummy.vars, newdata = col_ts) #apply to test data
  j=which(colnames(tr2)==cats[i]) #to locate which column would be replaced by dummy columns
  tr2=cbind(tr2[,1:(j-1)],col.dummy,tr2[,(j+1):dim(tr2)[2]]) #replace original column by dummies
  
  #using the approach of cbind()
  ts2=cbind(ts2[,1:(j-1)],col_ts.dummy,ts2[,(j+1):dim(ts2)[2]]) #apply to test data
  tr2=tr2[,-"target"]
  ts2=ts2[,-"target"]
}
train3=tr2[,-"ProductName"] # "ProductName" is the first column of the table,thus with j=1
test3=ts2[,-"ProductName"] # then the cbind() line above will not delete that column
# so we need to remove this column seperately
# One can surely code in a more elegant way
str(train3)



#Near zero variance
nzv = nearZeroVar(train3, saveMetrics = TRUE)
nzv[,3:4]

nzv_cols = nearZeroVar(train3)
train4 = subset(train3,select=-nzv_cols) #delete the NearZeroVar columns
test4 = subset(test3,select=-nzv_cols) #Apply the result of train to test
str(train4)

#Fill missing values
pre.impute = preProcess(train4, method = "medianImpute")
train5 = predict(pre.impute, train4)
test5 = predict(pre.impute, test4) #Apply the result of train to test
str(train5)

nn = colSums(is.na(train5) > 0)
mm = colSums(train5 == '', na.rm = T)
mm+nn

train100k_processed=train5
test100k_processed=test5
write.csv(train100k_processed, 'train100k_processed.csv', row.names = FALSE)
write.csv(test100k_processed, 'test100k_processed.csv', row.names = FALSE)

str(train5)





source("http://www.sthda.com/upload/rquery_cormat.r")
rquery.cormat(train5, type = 'full')

# 
# library("Hmisc")
# mydata.rcorr = rcorr(as.matrix(train5))
# mydata.rcorr[[r]]
# write.csv(mydata.rcorr[['r']], 'mydata.rcorr.csv', row.names = FALSE)
# 
# model1 = lm(HasDetections ~ . , data=train5)
# library(car)
# library(VIF)
# vif(lm(HasDetections ~ . , data=train5))


library(leaps)#ALL-SUBSETS REGRESSION-NO NEED FOR LASSO
sum(sapply(train100k_processed, is.character))
fsr_model<- regsubsets(HasDetections~.,train100k_processed ,really.big = T, method='forward')#,matrix.logical=TRUE wrongsummary(fsr_model)
summary(fsr_model)
bestvariables <-names(which(summary(fsr_model)$which[20,]))




# 
# regfit.full<- regsubsets(HasDetections~.,train100k_processed ,nvmax=20,method='forward')#,matrix.logical=TRUE wrongsummary(fsr_model)
# bestvariables <-names(which(summary(regfit.full)$which[20,]))
# bestvariables
# reg.summary=summary(regfit.full)
# names(reg.summary)
# #1.??????????????????????????????RSS?????????R??????Cp??????BIC???????????????????????????
# par(mfrow=c(2,2))
# plot(reg.summary$rss,xlab = 'Number of Variables',ylab = 'RSS',type='l')
# plot(reg.summary$adjr2,xlab = 'Number of Variables',ylab = 'Adjusted RSq',type='l')
# which.max(reg.summary$adjr2)
# points(11,reg.summary$adjr2[11],col='red',cex=2,pch=20)
# plot(reg.summary$cp,xlab='number of variables',ylab='Cp',type='l')
# which.min(reg.summary$cp)
# points(10,reg.summary$cp[10],col='red',cex=2,pch=20)
# plot(reg.summary$bic,xlab='number of variables',ylab='Bic',type='l')
# which.min(reg.summary$bic)
# points(6,reg.summary$bic[6],col='red',cex=2,pch=20)
# 
# set.seed(1)
# train=sample(c(TRUE,FALSE),nrow(train5),rep=TRUE)
# test=(!train)
# regfit.best=regsubsets(HasDetections~.,data=train5[train,],nvmax=20,really.big=T) #????????????????????????
# summary(regfit.best)
# test.mat=model.matrix(HasDetections~.,data=train5[test,])
# val.errors=rep(NA,20)
# for(i in 1:20){
#   coefi=coef(regfit.best,id=i) # i?????????????????????
#   pred=test.mat[,names(coefi)]%*%coefi
#   val.errors[i]=mean((train5$HasDetections[test]-pred)^2)
# }
# val.errors
# which.min(val.errors)
